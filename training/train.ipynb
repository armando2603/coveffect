{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, CTRLLMHeadModel, GPT2TokenizerFast, CTRLTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, type, tokenizer, maxlen):\n",
    "\n",
    "        # Store the contents of the file in a pandas dataframe\n",
    "        self.df = pd.read_csv('data/dataset_v1.tsv', sep='\\t')\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inp_str = self.df.loc[index, 'abstract']\n",
    "        trg_str = self.df.loc[index, 'output']\n",
    "        # cond_str = self.df.loc[index, 'attribute']\n",
    "        inp_ids = self.tokenizer.encode(inp_str, truncation=True, max_length=1000)\n",
    "        # cond_str = self.tokenizer.encode(cond_str + ':')\n",
    "        trg_ids = self.tokenizer.encode(trg_str)\n",
    "        pad_id = int(self.tokenizer.pad_token_id)\n",
    "        bos_id = int(self.tokenizer.bos_token_id)\n",
    "        eos_id = int(self.tokenizer.eos_token_id)\n",
    "        sep_id = int(self.tokenizer.sep_token_id)\n",
    "        # sepo_id = int(self.tokenizer('<SEPO>'))\n",
    "\n",
    "        src = [bos_id] + inp_ids + [sep_id] + trg_ids + [eos_id]\n",
    "\n",
    "        if len(src) > self.maxlen:\n",
    "            inp_ids = inp_ids[:-(len(src) - self.maxlen)]\n",
    "            inp_ids = [bos_id] + inp_ids + [sep_id]\n",
    "            src = inp_ids + trg_ids + [eos_id]\n",
    "\n",
    "        src = src +\\\n",
    "            [pad_id for _ in range(self.maxlen - len(src))]\n",
    "\n",
    "        attn_mask = (torch.tensor(src) != pad_id)\n",
    "        # attn_mask[len(inp_ids):len(inp_ids + trg_ids)] = attn_mask_outputs\n",
    "\n",
    "        labels = torch.tensor(src)\n",
    "        labels[:len(inp_ids)] = torch.ones((len(inp_ids))) * -100\n",
    "        labels[labels == pad_id] = -100\n",
    "\n",
    "        assert src.count(bos_id) == 1, f'bos_id missing {src[:3]}'\n",
    "        assert src.count(eos_id) == 1, 'eos_id missing'\n",
    "        assert src.count(sep_id) == 1, 'sep_id missing'\n",
    "        # assert src.count(sepo_id) == 13, 'sepo_id missing'\n",
    "        return torch.tensor(src), attn_mask, labels\n",
    "\n",
    "\n",
    "class LM(pl.LightningModule):\n",
    "    def __init__(self, max_len: int=1000):\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "        ######### CHANGED ##########\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"mrm8488/GPT-2-finetuned-CORD19\", return_dict=True)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"mrm8488/GPT-2-finetuned-CORD19\")\n",
    "        self.tokenizer.add_special_tokens({\n",
    "            'pad_token': '<PAD>',\n",
    "            'bos_token': '<BOS>',\n",
    "            'eos_token': '<EOS>',\n",
    "            'sep_token': '<SEP>',\n",
    "            'additional_special_tokens': ['<SEPO>']\n",
    "\n",
    "        })\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        # checkpoint = torch.load('checkpoint-5-encode-15+-epoch=17-val_loss=0.035.ckpt')\n",
    "        # if 'state_dict' in checkpoint.keys():\n",
    "        #     state_dict = checkpoint['state_dict']\n",
    "        #     new_state_dict = OrderedDict()\n",
    "        #     for k, v in state_dict.items():\n",
    "        #         if k[:6] == 'model.':\n",
    "        #             name = k[6:]\n",
    "        #         else:\n",
    "        #             name = k\n",
    "        #         new_state_dict[name] = v\n",
    "        #     self.model.load_state_dict(new_state_dict)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inp, mask, labels = batch\n",
    "        return self.model(inp, attention_mask=mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.forward(batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     output = self.forward(batch)\n",
    "    #     loss = output.loss\n",
    "    #     self.log('val_loss', loss, prog_bar=True)\n",
    "    #     return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = CustomDataset('train', self.tokenizer, self.max_len)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=2, num_workers=2, shuffle=True)\n",
    "\n",
    "        return train_dataloader\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     val_dataset = CustomDataset('validation', self.tokenizer, self.max_len)\n",
    "\n",
    "    #     val_dataloader = torch.utils.data.DataLoader(\n",
    "    #     val_dataset, batch_size=12, num_workers=2, shuffle=False)\n",
    "\n",
    "    #     return val_dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os.path\n",
    "    from os import path\n",
    "    save_dir_path = \"/content/Checkpoints\"\n",
    "    if not path.exists(save_dir_path):\n",
    "        os.mkdir(save_dir_path)\n",
    "    drive_save_dir_path = \"/content/drive/MyDrive/Muteffstage/Checkpoints\"\n",
    "    if not path.exists(drive_save_dir_path):\n",
    "        os.mkdir(drive_save_dir_path)\n",
    "    model = LM()\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=save_dir_path,\n",
    "        # monitor='val_loss',\n",
    "        filename='multiple-instances-cord19-{epoch:02d}',\n",
    "        save_top_k=1,\n",
    "        # mode='min',\n",
    "        verbose=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    # early_stopping_callback = EarlyStopping(\n",
    "    #     monitor='val_loss',\n",
    "    #     patience=2,\n",
    "    #     mode='min',\n",
    "    #     verbose=True\n",
    "    # )\n",
    "    trainer = Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=8,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "           checkpoint_callback,\n",
    "        #    early_stopping_callback\n",
    "        ]\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "    from distutils.dir_util import copy_tree\n",
    "    copy_tree(\"/content/Checkpoints\", \"Checkpoints\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
