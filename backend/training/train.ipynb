{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, CTRLLMHeadModel, GPT2TokenizerFast, CTRLTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, type, tokenizer, maxlen):\n",
    "\n",
    "        # Store the contents of the file in a pandas dataframe\n",
    "        self.df = pd.read_csv('data/dataset_v1.tsv', sep='\\t')\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inp_str = self.df.loc[index, 'abstract']\n",
    "        trg_str = self.df.loc[index, 'output']\n",
    "        # cond_str = self.df.loc[index, 'attribute']\n",
    "        inp_ids = self.tokenizer.encode(inp_str, truncation=True, max_length=1000)\n",
    "        # cond_str = self.tokenizer.encode(cond_str + ':')\n",
    "        trg_ids = self.tokenizer.encode(trg_str)\n",
    "        pad_id = int(self.tokenizer.pad_token_id)\n",
    "        bos_id = int(self.tokenizer.bos_token_id)\n",
    "        eos_id = int(self.tokenizer.eos_token_id)\n",
    "        sep_id = int(self.tokenizer.sep_token_id)\n",
    "        # sepo_id = int(self.tokenizer('<SEPO>'))\n",
    "\n",
    "        src = [bos_id] + inp_ids + [sep_id] + trg_ids + [eos_id]\n",
    "\n",
    "        if len(src) > self.maxlen:\n",
    "            inp_ids = inp_ids[:-(len(src) - self.maxlen)]\n",
    "            inp_ids = [bos_id] + inp_ids + [sep_id]\n",
    "            src = inp_ids + trg_ids + [eos_id]\n",
    "\n",
    "        src = src +\\\n",
    "            [pad_id for _ in range(self.maxlen - len(src))]\n",
    "\n",
    "        attn_mask = (torch.tensor(src) != pad_id)\n",
    "        # attn_mask[len(inp_ids):len(inp_ids + trg_ids)] = attn_mask_outputs\n",
    "\n",
    "        labels = torch.tensor(src)\n",
    "        labels[:len(inp_ids)] = torch.ones((len(inp_ids))) * -100\n",
    "        labels[labels == pad_id] = -100\n",
    "\n",
    "        assert src.count(bos_id) == 1, f'bos_id missing {src[:3]}'\n",
    "        assert src.count(eos_id) == 1, 'eos_id missing'\n",
    "        assert src.count(sep_id) == 1, 'sep_id missing'\n",
    "        # assert src.count(sepo_id) == 13, 'sepo_id missing'\n",
    "        return torch.tensor(src), attn_mask, labels\n",
    "\n",
    "\n",
    "class LM(pl.LightningModule):\n",
    "    def __init__(self, max_len: int=1000):\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "        ######### CHANGED ##########\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(\"mrm8488/GPT-2-finetuned-CORD19\", return_dict=True)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"mrm8488/GPT-2-finetuned-CORD19\")\n",
    "        self.tokenizer.add_special_tokens({\n",
    "            'pad_token': '<PAD>',\n",
    "            'bos_token': '<BOS>',\n",
    "            'eos_token': '<EOS>',\n",
    "            'sep_token': '<SEP>',\n",
    "            'additional_special_tokens': ['<SEPO>']\n",
    "\n",
    "        })\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        # checkpoint = torch.load('checkpoint-5-encode-15+-epoch=17-val_loss=0.035.ckpt')\n",
    "        # if 'state_dict' in checkpoint.keys():\n",
    "        #     state_dict = checkpoint['state_dict']\n",
    "        #     new_state_dict = OrderedDict()\n",
    "        #     for k, v in state_dict.items():\n",
    "        #         if k[:6] == 'model.':\n",
    "        #             name = k[6:]\n",
    "        #         else:\n",
    "        #             name = k\n",
    "        #         new_state_dict[name] = v\n",
    "        #     self.model.load_state_dict(new_state_dict)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inp, mask, labels = batch\n",
    "        return self.model(inp, attention_mask=mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.forward(batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     output = self.forward(batch)\n",
    "    #     loss = output.loss\n",
    "    #     self.log('val_loss', loss, prog_bar=True)\n",
    "    #     return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = CustomDataset('train', self.tokenizer, self.max_len)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=2, num_workers=2, shuffle=True)\n",
    "\n",
    "        return train_dataloader\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     val_dataset = CustomDataset('validation', self.tokenizer, self.max_len)\n",
    "\n",
    "    #     val_dataloader = torch.utils.data.DataLoader(\n",
    "    #     val_dataset, batch_size=12, num_workers=2, shuffle=False)\n",
    "\n",
    "    #     return val_dataloader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os.path\n",
    "    from os import path\n",
    "    save_dir_path = \"/content/Checkpoints\"\n",
    "    if not path.exists(save_dir_path):\n",
    "        os.mkdir(save_dir_path)\n",
    "    drive_save_dir_path = \"/content/drive/MyDrive/Muteffstage/Checkpoints\"\n",
    "    if not path.exists(drive_save_dir_path):\n",
    "        os.mkdir(drive_save_dir_path)\n",
    "    model = LM()\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=save_dir_path,\n",
    "        # monitor='val_loss',\n",
    "        filename='multiple-instances-cord19-{epoch:02d}',\n",
    "        save_top_k=1,\n",
    "        # mode='min',\n",
    "        verbose=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    # early_stopping_callback = EarlyStopping(\n",
    "    #     monitor='val_loss',\n",
    "    #     patience=2,\n",
    "    #     mode='min',\n",
    "    #     verbose=True\n",
    "    # )\n",
    "    trainer = Trainer(\n",
    "        gpus=1,\n",
    "        max_epochs=8,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "           checkpoint_callback,\n",
    "        #    early_stopping_callback\n",
    "        ]\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "    from distutils.dir_util import copy_tree\n",
    "    copy_tree(\"/content/Checkpoints\", \"Checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Missing logger folder: /home/ubuntu/virus-project/backend-gpu/training/lightning_logs\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/ubuntu/anaconda3/envs/data-science/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 154/154 [00:28<00:00,  5.34it/s, loss=0.222, v_num=0, train_loss_step=0.216, val_loss_step=0.151, val_loss_epoch=0.141, train_loss_epoch=0.216]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from simplet5 import SimpleT5\n",
    "\n",
    "# self.df = pd.read_csv('data/dataset_v1.tsv', sep='\\t')\n",
    "df_train = pd.read_csv('dataset_v2.tsv', sep='\\t')\n",
    "df_train = df_train.rename(columns={'output':\"target_text\", 'abstract':\"source_text\"})\n",
    "df_train = df_train[['source_text', 'target_text']]\n",
    "\n",
    "# df_dev = pd.read_csv('../data/trainingdata_v3/datasets/train_text_splitted750_entities.tsv', sep='\\t')\n",
    "# df_dev = df_dev.rename(columns={\"entities\":\"target_text\", \"text\":\"source_text\"})\n",
    "# df_dev = df_dev[['source_text', 'target_text']]\n",
    "# df_dev['source_text'] = \"medications: \" + df_dev['source_text']\n",
    "# df_dev.loc[pd.isna(df_dev['target_text'])] = ' '\n",
    "# print(df_train)\n",
    "model = SimpleT5()\n",
    "model.from_pretrained(model_type=\"t5\", model_name=\"manueldeprada/t5-cord19\")\n",
    "\n",
    "model.train(\n",
    "    train_df=df_train,\n",
    "    eval_df = df_train,\n",
    "    source_max_token_len=600, \n",
    "    target_max_token_len=300, \n",
    "    batch_size=2,\n",
    "    max_epochs=7,\n",
    "    use_gpu=True,\n",
    "    save_only_last_epoch=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d12880202bc95959d0d60b6e67cff9b367f68df1e17e4b553464e9803ea38e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('data-science': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
