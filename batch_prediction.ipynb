{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model_path) -> None:\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.status = 0\n",
    "    \n",
    "        self.hparam = {\n",
    "            'max_len': 1000,\n",
    "        }\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.greek_names = [\n",
    "            'ALPHA',\n",
    "            'BETA',\n",
    "            'GAMMA',\n",
    "            'DELTA',\n",
    "            'EPSILON',\n",
    "            'ZETA',\n",
    "            'ETA',\n",
    "            'THETA',\n",
    "            'IOTA',\n",
    "            'KAPPA',\n",
    "            'LAMBDA',\n",
    "            'MI',\n",
    "            'NI',\n",
    "            'XI',\n",
    "            'OMICRON',\n",
    "            'PI',\n",
    "            'RHO',\n",
    "            'SIGMA',\n",
    "            'TAU',\n",
    "            'HYPSILON',\n",
    "            'PHI',\n",
    "            'CHI',\n",
    "            'PSI',\n",
    "            'OMEGA',\n",
    "        ]\n",
    "        self.effects_names = [\n",
    "            'binding_to_antibodies',\n",
    "            'binding_to_host_receptor',\n",
    "            'ct_value',\n",
    "            'disease_severity',\n",
    "            'effectiveness_of_available_antiviral_drugs',\n",
    "            'effectiveness_of_available_diagnostics',\n",
    "            'effectiveness_of_available_vaccines',\n",
    "            'entry_efficiency',\n",
    "            'fatality_rate',\n",
    "            'host_virus_interactions',\n",
    "            'immune_escape',\n",
    "            'infection_duration',\n",
    "            'infectivity',\n",
    "            'intermolecular_interactions',\n",
    "            'protein_conformational_optimization',\n",
    "            'protein_flexibility',\n",
    "            'protein_functioning',\n",
    "            'protein_stability',\n",
    "            'risk_of_hospitalization',\n",
    "            'risk_of_reinfection',\n",
    "            'sensitivity_to_convalescent_sera',\n",
    "            'sensitivity_to_antibodies',\n",
    "            'sensitivity_to_vaccinated_sera',\n",
    "            'viral_fitness',\n",
    "            'viral_incubation_period',\n",
    "            'viral_load',\n",
    "            'viral_replication',\n",
    "            'viral_transmission',\n",
    "            'viral_virulence',\n",
    "        ]\n",
    "    \n",
    "    def generate(self, input_ids):\n",
    "        with torch.no_grad():\n",
    "            generated_sequence = []\n",
    "            distributions = []\n",
    "            comma_id = self.tokenizer.encode(',')[0]\n",
    "            eos_id = self.tokenizer.eos_token_id\n",
    "            sep_id = self.tokenizer.sep_token_id\n",
    "            sepo_id = self.tokenizer(' | ')['input_ids'][0]\n",
    "            output_indexes = [0]\n",
    "            current_index = 0\n",
    "            # past = None\n",
    "            ended_with_eos = False\n",
    "            while(len(generated_sequence) < 100):\n",
    "\n",
    "                if current_index > 0:\n",
    "                    if (predicted_token_tensor == comma_id):\n",
    "                        output_indexes.append(current_index)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids,\n",
    "                    # past_key_values=past,\n",
    "                    # use_cache=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                # past = outputs.past_key_values\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                predicted_token_tensor = torch.argmax(next_token_logits)\n",
    "\n",
    "                if (predicted_token_tensor == eos_id) or (predicted_token_tensor == sepo_id) or (predicted_token_tensor == sep_id) :\n",
    "                    ended_with_eos = True\n",
    "                    break\n",
    "\n",
    "                distributions.append(\n",
    "                    F.softmax(next_token_logits[0], 0).detach()\n",
    "                )\n",
    "                \n",
    "                input_ids = torch.cat(\n",
    "                    (input_ids, predicted_token_tensor.view(1, 1).detach()),\n",
    "                    dim=-1\n",
    "                )\n",
    "                generated_sequence.append(predicted_token_tensor.detach())\n",
    "                current_index += 1\n",
    "            return generated_sequence, output_indexes, distributions, ended_with_eos\n",
    "\n",
    "    def generateTable(self, inputs, output_attributes, pre_table_outputs=None):\n",
    "        self.status = 0\n",
    "        table_outputs = []\n",
    "        fields = output_attributes\n",
    "        with torch.no_grad():\n",
    "            for it, (input_text, doi) in enumerate(tqdm(inputs)):\n",
    "                # print(input_text)\n",
    "                self.status = round((it + 1)/len(inputs), 2) * 100\n",
    "\n",
    "                prefix_input_ids = self.tokenizer.encode(\n",
    "                    input_text,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=self.hparam['max_len'] -100\n",
    "                )\n",
    "                if pre_table_outputs != None:\n",
    "                    generated_outputs = pre_table_outputs[doi]\n",
    "                    if generated_outputs == [[]]:\n",
    "                        table_outputs.append(dict(doi=doi, outputs=[]))\n",
    "                        continue\n",
    "\n",
    "                else:\n",
    "                    generated_outputs = [[]]\n",
    "                    \n",
    "                for field in fields:\n",
    "                    tmp_generated_outputs = []\n",
    "                    for istance_index, instance in enumerate(generated_outputs):\n",
    "                        confidences = []\n",
    "                        past_conditional = ''\n",
    "                        if len(instance) > 0:\n",
    "                            for output in instance:\n",
    "                                past_conditional += output['attribute'] + ': ' + output['value'] + ' | '\n",
    "                        tmp_field = field['value']\n",
    "                        conditional_text = past_conditional + tmp_field + '_list:' if field['multiple'] else past_conditional + tmp_field + ':',\n",
    "                        # print('conditional text:', conditional_text[0])\n",
    "                        \n",
    "                        conditional_ids = self.tokenizer.encode(\n",
    "                            conditional_text[0],\n",
    "                            return_tensors='pt',\n",
    "                            truncation=True,\n",
    "                            max_length=100\n",
    "                        )\n",
    "\n",
    "\n",
    "                        input_ids = torch.cat(\n",
    "                            (\n",
    "                                torch.tensor([[self.tokenizer.bos_token_id]]),\n",
    "                                prefix_input_ids,\n",
    "                                torch.tensor([[self.tokenizer.sep_token_id]]),\n",
    "                                conditional_ids\n",
    "                            ),\n",
    "                            dim=-1\n",
    "                        ).to(self.device)\n",
    "\n",
    "                        generated_sequence, output_indexes, distributions, ended_with_eos\\\n",
    "                            = self.generate(input_ids)\n",
    "\n",
    "                        distributions = [\n",
    "                            distribution.cpu().numpy() for distribution in distributions\n",
    "                        ]\n",
    "                        # print(self.tokenizer.decode(generated_sequence))\n",
    "                        outputs_text = self.tokenizer.decode(generated_sequence).split(',')\n",
    "\n",
    "                        if not ended_with_eos:\n",
    "                            outputs_text = outputs_text[:-1]\n",
    "                            output_indexes = output_indexes[:-1]\n",
    "                        # print('Abstract number:', it)\n",
    "                        # print('generated sequence:', self.tokenizer.decode(generated_sequence))\n",
    "\n",
    "                        try:\n",
    "                            outputs_text_filtered = []\n",
    "                            for i, output_index in enumerate(output_indexes):\n",
    "                                # confidence 1st token\n",
    "                                if outputs_text[i] not in outputs_text[:i]:\n",
    "                                    if field['multiple']:\n",
    "                                        if self.output_is_valid(outputs_text[i].strip(), field['value']):\n",
    "                                            outputs_text_filtered.append(outputs_text[i].strip())\n",
    "                                            out_prob = distributions[output_index]\n",
    "                                            confidences.append(np.max(out_prob))\n",
    "                                    else:\n",
    "                                        outputs_text_filtered.append(outputs_text[i].strip())\n",
    "                                        out_prob = distributions[output_index]\n",
    "                                        confidences.append(np.max(out_prob))\n",
    "\n",
    "                                    # start_index = output_index\n",
    "                                    # end_index = output_indexes[i + 1] - 1 if i < (len(outputs_text)-1) else len(generated_sequence)\n",
    "\n",
    "                            assert len(outputs_text_filtered) == len(confidences), \\\n",
    "                                f'n of outputs not correspond n of confidences: {outputs_text} {confidences}\\n'\\\n",
    "                                +f'Len Input: {prefix_input_ids.shape}, Len Cond: {conditional_ids.shape}'\n",
    "\n",
    "                            if len(outputs_text_filtered) == 0 and field['value'] == 'mutation_name' and field['multiple'] :\n",
    "                                tmp_generated_outputs = []\n",
    "                                break\n",
    "\n",
    "                        except:\n",
    "                            print(outputs_text)\n",
    "                            print('an error occurs... skip and keep running...')\n",
    "                            break\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        for output_index, output in enumerate(outputs_text_filtered):\n",
    "                            if output not in outputs_text[:output_index]:\n",
    "                                tmp_generated_outputs.append(\n",
    "                                    instance + [\n",
    "                                        dict(\n",
    "                                            attribute=field['value'],\n",
    "                                            value=output.strip(),\n",
    "                                            confidence=np.round(np.float64(confidences[output_index]), 2),\n",
    "                                        )\n",
    "                                    ]\n",
    "                                )\n",
    "                    \n",
    "                    generated_outputs = tmp_generated_outputs\n",
    "                \n",
    "                table_outputs.append(dict(doi=doi, outputs=generated_outputs))\n",
    "            return table_outputs\n",
    "\n",
    "\n",
    "    def output_is_valid(self, output, attribute):\n",
    "        if attribute == 'mutation_name':\n",
    "            if re.search('_', output):\n",
    "                if re.search('^([A-Z0-9]+_)[A-Z]\\d{1,4}[A-Z]$', output):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                if re.search('\\.', output):\n",
    "                    if re.search('^([A-Z]{1,2}\\.[0-9]{1,3})(\\.[0-9]{1,3}){,2}$', output): \n",
    "                        return True\n",
    "                    else:\n",
    "                        return False\n",
    "                else:\n",
    "                    if output in self.greek_names:\n",
    "                        return True\n",
    "                    else:\n",
    "                        return False\n",
    "        if attribute == 'effect':\n",
    "            if output in self.effects_names:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Predictor('backend/api/checkpoints/model_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert here your list of abstract and the doi list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list = [\n",
    "    'The recently reported B.1.1.529 Omicron variant of SARS-CoV-2 includes 34 mutations in the spike protein relative to the Wuhan strain that initiated the COVID-19 pandemic, including 15 mutations in the receptor binding domain (RBD). Functional studies have shown omicron to substantially escape the activity of many SARS-CoV-2-neutralizing antibodies. Here we report a 3.1 Å resolution cryo-electron microscopy (cryo-EM) structure of the Omicron spike protein ectodomain. The structure depicts a spike that is exclusively in the 1-RBD-up conformation with increased mobility and inter-protomer asymmetry. Many mutations cause steric clashes and/or altered interactions at antibody binding surfaces, whereas others mediate changes of the spike structure in local regions to interfere with antibody recognition. Overall, the structure of the omicron spike reveals how mutations alter its conformation and explains its extraordinary ability to evade neutralizing antibodies. Highlights SARS-CoV-2 omicron spike exclusively adopts 1-RBD-up conformation Omicron substitutions alter conformation and mobility of RBD A subset of omicron mutations change the local conformation of spike The structure reveals the basis of antibody neutralization escape'\n",
    "]\n",
    "doi_list = [\n",
    "    '10.1101/2021.12.21.473620'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abstract_doi_list = list(zip(abstract_list, doi_list))\n",
    "prediction_attributes = [\n",
    "    { 'value': 'mutation_name', 'multiple': True},\n",
    "    { 'value': 'effect', 'multiple': True},\n",
    "    { 'value': 'level', 'multiple': False}\n",
    "]\n",
    "pre_table_outputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [01:04<00:00, 64.46s/it]\r100%|██████████| 1/1 [01:04<00:00, 64.47s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction_results = pred.generateTable(abstract_doi_list, prediction_attributes, pre_table_outputs=pre_table_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame()\n",
    "for result in prediction_results:\n",
    "    if len(result['outputs']) > 0:\n",
    "        for instance in result['outputs']:\n",
    "            output_dict = {}\n",
    "            output_dict['doi'] = [result['doi']]\n",
    "            for output in instance:\n",
    "                output_dict[output['attribute']] = [output['value']]\n",
    "                \n",
    "            df_prediction = pd.DataFrame(output_dict)\n",
    "            df_predictions = pd.concat([df_predictions, df_prediction], ignore_index=True, axis=0)\n",
    "    else:\n",
    "        output_dict = dict(doi=[result['doi']], mutation_name=[''], effect=[''], level=[''])\n",
    "        # print(output_dict)\n",
    "        df_prediction = pd.DataFrame(output_dict)\n",
    "        df_predictions = pd.concat([df_predictions, df_prediction], ignore_index=True, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>mutation_name</th>\n",
       "      <th>effect</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty_doi</td>\n",
       "      <td>OMICRON</td>\n",
       "      <td>binding_to_antibodies</td>\n",
       "      <td>lower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>empty_doi</td>\n",
       "      <td>OMICRON</td>\n",
       "      <td>protein_conformational_optimization</td>\n",
       "      <td>no evidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>empty_doi</td>\n",
       "      <td>OMICRON</td>\n",
       "      <td>immune_escape</td>\n",
       "      <td>undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi mutation_name                               effect        level\n",
       "0  empty_doi       OMICRON                binding_to_antibodies        lower\n",
       "1  empty_doi       OMICRON  protein_conformational_optimization  no evidence\n",
       "2  empty_doi       OMICRON                        immune_escape    undefined"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coveffect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "892c17546e98cafe553fb5b11e5f3d3647269577cb8dc62c19386cd93073ab35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
